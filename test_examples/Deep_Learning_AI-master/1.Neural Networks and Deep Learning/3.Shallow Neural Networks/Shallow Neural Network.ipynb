{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply a stack of single neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an input layer, a hidden layer, and an output layer. The values in the hidden layer are not observed.\n",
    "\n",
    "The input layer does not count in the number of layers. We only count the number of hidden layers and add the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tanh` is usually a better activation function than he sigmoid function. However, if the ouput is binary, the sigmoid function makes more sense to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, both functions have the downside of having very small derivatives for large values of `x`. Therefore, the rectified linear unit (ReLU) can also be used.\n",
    "\n",
    "ReLU = max(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why non-linear activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't get it... Need to look somewhere else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot initalize weight to 0, because all activations will be the same. Likely, the derivatives will be the same. Therefore, the hidden units will be symmetric (completely identic). \n",
    "\n",
    "Hence, they compute the same activation. Therefore, no matter how we train the network, it will always compute the same output.\n",
    "\n",
    "That's why you must initialize the weights randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
