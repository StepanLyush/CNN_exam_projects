{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Machine Learning Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ML Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge with deep learning is that there are many ways to improve a model:\n",
    "\n",
    "* Gather more data\n",
    "* Train the algorithm for a longer time\n",
    "* Change the architecture of the neural network\n",
    "* Get more diverse training set\n",
    "\n",
    "However, pursuing the wrong strategy can result in an importat loss of time and resources. You could be spending 6 months gathering more data only to realize that it barely improves the model. \n",
    "\n",
    "Therefore, it is important to have a good strategy when it comes to managing and improving machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most effictive ML practitioners have a clear view of what parameters to tune in order to achieve a better result.\n",
    "\n",
    "Orthogonalization refers to having a control with a very specific function. A single control should impact only one variable or parameter. That way, it is much easier to tune to achieve an expected result. If multiple controls can affect a  single variable, then it is much harder to achieve an optimal result.\n",
    "\n",
    "**Show orthogonal graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that translate to machine learning?\n",
    "\n",
    "We need to consider the chain of assumptions in machine learning. It is assumed that if the model performs well on the training set, then it will perform well on the dev set and on the test set. Similarly, it should therefore perform well in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training: bigger network or change optimization algorithm\n",
    "* Dev set: use regularization or bigger train set\n",
    "* Test set: use a bigger dev set\n",
    "* Real world: Change dev distribution set or change cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single number evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a single number evaluation metric allows for quicker assessment of an algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for a classifier, precision and recall are common evaluation metrics. However, there is a tradeoff between these two metrics. Instead, we should use a metric that combines both.\n",
    "\n",
    "In this case, we use the F1 score, which represents the harmonic mean of precision and recall. That way, it is much easier to assess the quality of different models, and it speeds up iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satisficing and optimizing metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are concerned by both the accuracy and running time of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you would like to maximize accuracy while keeping the running time small (say less than 100ms). Therefore, the accuracy is the *optimizing* metric and the running time is the *satisficing* metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if there are many metrics that you wish to consider, 1 should be an optimizing metric, and the rest should be satisficing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train/dev/test distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way these sets are set up can really make a difference between slowing down a team or increasing its efficiency and speed of iterations towards the right direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aa a general guideline, the dev set and test set should reflect the data that you expect to get in the future and consider to do well on. \n",
    "\n",
    "For example, a credit modelling alogrithm should not be trained on low income instances if it is to be deployed with medium-income  individuals.\n",
    "\n",
    "In other words, they must come from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of dev and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How large should they be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old way of doing it was a 70/30 train/test split or 60/20/20 for train/dev/test.\n",
    "\n",
    "This is still valid in the case where data is not abundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in this new era of deep learning, huge amounts of data are available. Therefore, it is better to do a 98/1/1 train/test/dev split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set, it should be big enough to give high confidence in the overall performance of the system. This could be 10 000 or 100 000 examples that would represent less than 10% of the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to human-level performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why human-level performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last few years, we have been comparing machine learning systems to human-level performance. This is possible, because the performance of many of these systems are very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, progress is fast as it approaches human-level performance. After though, progress slows down until it reaches the bayes optimal error. This is the error where no possible function can be accurate at a 100%. For example, a picture is too blurry or a sound sample is too noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, humans are very good at a lot of tasks involving natural data. While your algorithm is not as good as humans you can:\n",
    "* get labeled data from humans\n",
    "* perform manual error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoidable bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human performance is a good estimate of the Bayes error. Now, if the training error is far for the human-level performance, then you should focus on reducing bias and increasing the performance on the training set.\n",
    "\n",
    "However, if on the training set, the system performs closely to human-level, then you should focus on reducing variance and improving performance on the dev set error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The avoidable bias is the difference between human-level performance and the algorithm's bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to improve a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce bias:\n",
    "* train bigger model\n",
    "* train longer/better optimization algorithm(Adam, momentum)\n",
    "* Change NN architecture/hyperparameters (use RNN or CNN)\n",
    "\n",
    "Reduce bias:\n",
    "* more data\n",
    "* regularization (L2, dropout, data augmentation)\n",
    "* Change NN architecture/hyperparameters (use RNN or CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrying out error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the errors and determine what would be the best outcome.\n",
    "\n",
    "For example, 50 images were misclassified out of a 100, then your error rate will improve by 50%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up incorrectly labeled examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DL algorithms are quite robust to random errors in random set. Not worth correcting if the amount of errors is small and random. However, they will suffer with systematic errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply same process to dev/test set to make sure they continue to come from the same distribution\n",
    "* Consider examining examples the algo got right and wrong\n",
    "* Train and dev/test data may now come from slightly different distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build quickly and iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't build something too simple or too complex. Buid fast and use error anaylsis to iterate afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mismatched training and dev/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing on different distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
